{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4NlfEQhRWPy"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Summarization of text is an important task in Natural Language Processing and is also very useful for consumer enterprise. For example, there is a long written news article, or an article giving a review about a newly launched movie, or an article about newly opened shopping mall in the town, automatic generation of summary from such articles can not only cut short the time of person by non-requirement of reading the complete text but also it can help in doing sentiment analysis of the text. In the following Article Summarizer project, we are going to using attention model architecture to build and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CChWzW-rEHVb",
    "outputId": "a0b3e98b-7fc6-492d-c8ad-3a263b54f670"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import w2_tests\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trax                         1.3.9\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list|grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEL2rvaHRWP4"
   },
   "source": [
    "##  Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trax makes it easy to work with Tensorflow's datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VInmKSkhEhle"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7fc5bea0d8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7fc5bea0d8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7fc5bea0d8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7fc5bea0d3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7fc5bea0d3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7fc5bea0d3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Importing CNN/DailyMail articles dataset\n",
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and DeTokenization Function\n",
    "\n",
    "Given any data set, we need to map words to their indices, and indices to their words. The inputs and outputs to the Trax models are usually tensors of numbers where each number corresponds to a word.\n",
    "\n",
    "Here we will implement the following functions:\n",
    "\n",
    "tokenize: converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
    "\n",
    "detokenize: converts a token list to its corresponding sentence (i.e. string).\n",
    "def tokenize(input_str, EOS=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djTiSLcaNFGa"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, EOS=1):\n",
    "   \n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "  \n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WvhaFbCRWQS"
   },
   "source": [
    "## Preprocessing \n",
    "\n",
    "language models only predict the next word, they have no notion of inputs. To create a single input suitable for a language model, we concatenate inputs with targets putting a separator in between. We also need to create a mask -- with 0s at inputs and 1s at targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4rgPxYSRWQS"
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # Tokenizes\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/', vocab_file='summarize32k.subword.subwords'), preprocess, trax.data.FilterByLength(2048))\n",
    "\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "uKFoGsUKSa_I",
    "outputId": "bc4d6634-d716-4311-d49c-1956bca2bc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "S4uHyCkbSuUo",
    "outputId": "52845be8-f2fc-4803-bf7a-ed9725fe2bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " QPR chairman Tony Fernandes has insisted his club can afford not to\n",
      "win promotion to the Premier League, despite debts of £177.1 million.\n",
      "Rangers face Derby County in the Championship play-off final at\n",
      "Wembley on May 24, with Harry Redknapp's side hoping to secure the\n",
      "£120m pay packet of Premier League promotion. But, should QPR return\n",
      "to the top tier at the first attempt, they could be forced to pay out\n",
      "more than half of that in fines under the Football League's Financial\n",
      "Fair Play regulations. We're ready: Queens Park Rangers chairman Tony\n",
      "Fernandes says his club doesn't have to win promotion . Off to\n",
      "Wembley: Rangers won their way through to the play-off final after\n",
      "extra-time against Wigan . Based on last year's accounts, Rangers\n",
      "would have to pay £62.1m if they are promoted because their £65.4m\n",
      "losses were so far in excess of the £8m allowed by the Football\n",
      "League. Should Redknapp's side stay in the Championship, however, they\n",
      "would be subjected to a transfer embargo. QPR have tried to reduce\n",
      "their wage bill by selling high-earners such as Christopher Samba and\n",
      "sending the likes of Loic Remy and Adel Taarabt out on loan. Improved:\n",
      "Businessman Fernandes says QPR are in a better financial position than\n",
      "two years ago . Winner: QPR striker Charlie Austin finds the net to\n",
      "snatch the victory in the play-off second leg . Fernandes told\n",
      "talkSPORT: 'Yes, we can (afford not to go up). I'm an accountant by\n",
      "background - although I may not seem it! 'We've told the fans, whether\n",
      "we go up or we don't, we're here for the long term. 'We know what\n",
      "culture we want at the club and we will continue the journey whether\n",
      "we're in the Championship or the Premier League. 'We are a much\n",
      "smarter, much wiser group of people than we were two years ago.'\n",
      "Relief: QPR's bid to reduce their wage bills included off-loading\n",
      "players such  as Christopher Samba . Major break: Loic Remy (right)\n",
      "went on loan to Newcastle and scored 14 goals for Alan Pardew's side\n",
      ".<EOS><pad>QueensPark Rangers have debts of £177.1 million . The club\n",
      "is also set to be hit with fines under Financial Fair Play rules . But\n",
      "chairman Tony Fernandes says they can survive without winning\n",
      "promotion to the Premier League this season . QPR take on Derby in the\n",
      "play-off final at Wembley on May 24 .<EOS>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4sDS1WIVaYG"
   },
   "source": [
    "\n",
    "\n",
    "## Create Batches of Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqj1NsbERWQX"
   },
   "outputs": [],
   "source": [
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,    8,    4,    2, 1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6M5OA8QRWQb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1719)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch, _, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# Shape of the input_batch\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SjNOlljxTGuQ",
    "outputId": "9227c68c-6369-4ce8-8137-506c594f6ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   52    23    46  2663   285 16915  2846    78   213   927   145   213\n",
      " 17418  1287    78   213   184    10    59     3 14026   532   132  6178\n",
      "  7344  6758  3060  1065  1019  4215   320   399 12506  2475 14747   186\n",
      "  3060    25   793   320  1670   246     3  4142  1466   432   145   213\n",
      " 17418   635     6  1807  7726    18   127   285    28 16626   220  2418\n",
      "  1019   726  2479   640   213 16915   880   408   155  1287  1353  6503\n",
      "     2   188   536  6400  5689     6  8155  1582    25    86    72   926\n",
      "   463     3   244   103    23    46  2663    77  1353   444  2178   111\n",
      "   213 16915 16441     4   132  6178  7344  6758   186   213   184    10\n",
      "    59     3   726     2 13884   501  9291     5    78   213  4217  1945\n",
      "     7     5 17872     4   285    77  1793     7    26   811   215   320\n",
      " 21427     4  1221    70 20297    16   213  2854    95    31  8242   527\n",
      "   213  1287    78   493  8064   186    50 19432   379  9175  6051     4\n",
      "   246  1019   846   379 10401 10068    11    52    23    46  2663   592\n",
      "   285 16915 21148  1551   809   213  6178  7344  6758 23647   587 10071\n",
      " 10161    40    31  7658  1019   399  6503   145   213 17418  7726    78\n",
      "   493   253   379     9 24128    84  7726  1480   375   320   213   583\n",
      "   527 12506 14747   186   150    54   184    10    59     3  2013   559\n",
      "   809   128    10   668   142    10    75     3   412   213   184    10\n",
      "    59 14026   532   408   155  1072  1838 12380  8532  5874  1221     3\n",
      "  1324   320  4958  1652     2  1819     6  2507 19908   197  9573    36\n",
      " 12175     2  1779  1353   160   527    28   325   248   809   213 16915\n",
      " 16441     4  2685    28  3351  1838   213 23647   587     2  1065    15\n",
      " 23017     5   122    22   143   273   186  4149   213 13326  9481    21\n",
      " 12031 10071     3   342     2    41    25   793   320   117  1670   246\n",
      "    80   186   931   320  1466  1779  4276   320   213   782  2912    25\n",
      "   793   320   117  1670   246    80   457   102  4823  1019    28   270\n",
      "    55   320   399 12506 14747   186    15   801     3 19771 16279   140\n",
      "    97  2979     2 12175   186    72   469 23898  2741   133    31   138\n",
      "   320   213 23647   587  1480   691   169  1353  6199 19160    14   186\n",
      "   559  9917    78   213 23208     5     3 17169    11    52   229 12112\n",
      "  1779  6503   213  7658   527   213 16915  1019   707  1221  1161   379\n",
      "     9  4319  3212  1017  1480 12175  1353   160   527  1504   163 26059\n",
      "    78   527   213   548   474   186  7304   213   661   527   303   676\n",
      "   801   516 10940  1206  1779    40   710   132   213  1981  1287     3\n",
      "   342     2   213   248  1838   213 16915 16441     4   143    19 15676\n",
      " 12506 14747   186  1459   320    31   221  1160   809   239 23056  4872\n",
      "    41   408   155  1287   880     3 23773 20172   114  3588  1019  2479\n",
      "  1838 16587  6046  3653  1891  1160   132  2060  1480   229    72   926\n",
      "   463     2   103   229  2663   285    72  1776   707  1302  1161   186\n",
      "   908   291    25   793   320  3945    70  1786   213  2639  1867  6158\n",
      "    16  1019   290   926     3    52   229    19   249  1779  6503   213\n",
      "  2418  1019   399  1019   213 16915 21148  1551    78   213   927   809\n",
      "  6178  7344  6758     3   223  1023    97  2065    39 12039   114   425\n",
      "   213  7970   527   213   352  2846  1779    25  2292    78   213   927\n",
      "   132  6178  7344  6758     3 24021 14257    17  1019  1474 16266  1098\n",
      "  1019   213 23647   587   801     2   213    82   215   667  3885 21148\n",
      "  1551 10161  1958   320  4149   186   132   503    25  6503    31   221\n",
      "  7658  1019   872   399     3 15685    21    11 12506  6381 14747     8\n",
      "   346    12   710   355  5805 13962  4524   361     2   192  3056 10940\n",
      "  1206     8   231    12   710   132    28   379 16626  1867   379 11063\n",
      "   251    11  7057  2507 19908 25916     4 20349 25558  5597    20     8\n",
      "   346    12   186  9573    36 12175     8   231    12    25  1343   132\n",
      "    28 25373     4  1287   379  9823  6085   113   213  5980  2248 18329\n",
      "  4504 23979     7     5  1534  7356   285    77  1365  1793     7    26\n",
      "   811   215   320 17775 12979  3095 21427     4  1221   320 22573   809\n",
      "   213    55   527   213  1287     2  1466    78   213   927  1534   285\n",
      "  2178  1353   552   799   213  1287     3  4087     2    36   516   527\n",
      "   213 16915   248  1779  1353    78   213  5363   527   213 16441     4\n",
      "  1353   132  6259   527    28 23533   320  2213 15218  6083   176 15932\n",
      "    45   186 10161  6485  4449    61  1838    28 17254   891  5256   859\n",
      "   320   245    64   163 23208     4  9917 25373     5     3  1324   320\n",
      "  1466  4071  1248   213  1359     2   213 16778    40  3314  1242  1248\n",
      "   213  8532  5874 25373     4   248   186   132   659  1353   217   320\n",
      "  3892   340  2194  1838  4872   213 23647   587 23208     5    25  9917\n",
      "  1838     3  4817  6703   194 18329  4504 23979  2663   285    22   186\n",
      "   747  1852  9859 13354    20     2  4968   527   213   184    10    59\n",
      "     3   726     7     5  8495 19114   527  5885     2   186   747  7071\n",
      " 25554     2   570   527   213   184    10    59     3   726     7     5\n",
      "  1068  8899     2  1425    41  3161     7    26   117   547  1221   809\n",
      "   993   132   285  1359  2002    27 16915 21148  1551  2418  1019   163\n",
      "  4440     6 10838   264  5256   859   320   245   246    28  8532  5874\n",
      " 25373     4   586  1353  6503   379  8398 13188    79    11 14308 23208\n",
      "     5 12615    17  4223     5   527 23863  4463   186   284  6199 19160\n",
      "    14   213 23647   587     7     5 16889   379 18183    14    11     9\n",
      " 10071   408   155  1939 25373     4   186  5256  1072   145   213  1287\n",
      "     2  1480 10316   329   926 11969     7    56  2195   210    28   335\n",
      "   926   186   103  1353   456    95   171     2    33   288     2    51\n",
      "    40   213  1171   320   456   288  2754  1353  7147  3898   127  4504\n",
      " 23979     3  4142     2  4958  1652    23  2401   285    77    25    72\n",
      "   726 11519 15932    45   497   213 18131     5   527  6178  7344  6758\n",
      "   145   213  1287  1480    62    18    46   217   320 19100     4   608\n",
      "    55 15170     5   527   213  7726   320   184    10    59     3  1631\n",
      "   132   213  1251   638  1359   796   186   213 20633  7642     3  9573\n",
      "    36 12175   186   297   492  2507 19908   197 20349 25558  5597    20\n",
      "    25  1343   691    28 25373     4  5067   809    66    28    10    75\n",
      "     3  8532  5874    55     2    95   635   926   102   213  1287   559\n",
      "   186   141    36  1807   102  5042  1838   163   199 24574 26516   204\n",
      "  2523  1233  1838  8221 17499    40  7720     3    56    82   215  1041\n",
      "   412   699 26651  4217     7     5  1111   320   213  2860   132 22573\n",
      "    23   424    28 15065    93   739   132   213   800     6  4562   184\n",
      "    10    59     3  5560  1239     2  1248  3837  7683  5475  1097  2685\n",
      "    15  1945     7     5 22450 13742   186 15491     3  4217  5478    18\n",
      "   132   983  5754  9229   527   454 14320 21690 21597     5   132   163\n",
      "  1587   320  2218   351   694  1838   213   583   527    28   184    10\n",
      "    59     3 17263   186   213   150   469  1343   132   213  6178  7344\n",
      "  6758  1287     3 22229    45     2 23201    45   186  5256  1072    11\n",
      "    27  8967    26     6    64  1081   132   979   527   213   184    10\n",
      "    59     3 23647   587   379     9  7049   527   213   638   527 10615\n",
      "     2  3837   240 26698     2  1065   132    28  1541   320  4217    78\n",
      "  3112  2685   647   726  2677   186  3472    25  1763   117   145   186\n",
      "   132   213  3865 19432   527   213  8409  1287 10220  2728    33  3059\n",
      "  2754  2677    25  1129   320    33   181   109   801     2   186 11406\n",
      "   103  1494  3472    25    19  1325   320  1151  1575     6 20241     2\n",
      "   953  1847 13725 14065 26698  1065     3  5980  2248 18329  4504 23979\n",
      "   793 20633  7642 16235   285   184    10    59     3  1221    25    78\n",
      "    28 26417  5344   205   527 18340   674   166   527   213  8064  4959\n",
      "   527   213   493   253     2   725     2  2860    78   119   262   186\n",
      "   766   691    67 17664     3    34   213 19432   527   213  1287     2\n",
      "  4504 23979 17406 16235   285   213 20633  7642 13046    28  3182  7301\n",
      "  1380     6  8409  1098   248   320  8221 17499   186    40  2507  1936\n",
      "   236   213  1965  1782   244    51    25  2120   320  5224   320   116\n",
      " 15724 15272    20     3   244  2265    40  1221   132   219   320   124\n",
      "   285  3898    22   127     3  3482   300   248    11   187   213   184\n",
      "    10    59     3 23647   587   132  6178  7344  6758   379   408   155\n",
      "    28 17682  1287   220   984     2    28  7424   248   527  6400   379\n",
      "  2981  1353  6503   213  1171   320  4149   213 16915  1779    40   413\n",
      "   155  1287   379     9  1945  3640  7840   213  1740   320  9741    95\n",
      "   163  1380     6  2295   327   186   127   103  1353    19 21352 12900\n",
      "  1173     3  4217   186    54  1631    18   254   127   213  4287  1353\n",
      "    28 24806     4  8409  1287     3   184    10    59     3  2248   527\n",
      "   303 26864    20  5313    23  7840   213 14905  5938   320   117   213\n",
      " 25614     4   527   489  2002    27   303   676  3244   133   218   824\n",
      "   719  1606   285    72   926   102   213  1287    78   213   184    10\n",
      "    59     3 12031  1529 10071   132  6178  7344  6758     2   213   676\n",
      "     7     5  8517   636  9961  1631   809   549   184    10    59     3\n",
      "  2863   285    28 13139     4   260   316 26363     4    67     6 14002\n",
      "    28    40  2663  2066    78  3698   186  3004  1019   213  2860     3\n",
      "   184    10    59     3  1631     2   176  5313     2    78  3155   127\n",
      "   285   106  2372  3213   627   206    19  7602   800   760   527  1779\n",
      "  1353  1478  1019   213  2860     3     9   303   676    23   284    61\n",
      "   163  1314   943   933   320  6962   213  2129   186  1111   320   213\n",
      "  2860     3     9   184    10    59     3  4064  3885  2121    78  3112\n",
      "   127   103    39  1358 11439   627   132   594    70   102   213   594\n",
      "    99  5560  1373    70    78  1098   186  3885   660  1689   691   213\n",
      "   493   253  1287   132 22573    10     1     0 10401 10068  3094    82\n",
      "   666    78   213  7715   527   213 16915   809  6178  7344  6758   186\n",
      "   213   338   527   291    41    25   411 16346 27439  6774  1628   312\n",
      "   213 16915 16441     4   413   155  1287   213   352  2846    25  6503\n",
      "    28  2418  1019   726   399  1786    28  5689  8155   248   144    72\n",
      "   926   463   132  2060 16346 27439  6774  1628   198  1353   444  2178\n",
      "   111 21148  1551    78   213   927   186  6422    70  1248   213   985\n",
      "   320 23533  2213 15932    45     2  9062   181   707  1221   320  2443\n",
      "  6083  2104     1]\n"
     ]
    }
   ],
   "source": [
    "# print corresponding integer values\n",
    "print(input_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bu05ZwbWTE6P",
    "outputId": "3d455bd7-e343-4c25-a467-572d2abd837f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\n",
      " It has been claimed that CIA agents on the ground during the deadly\n",
      "attack on the U.S. Consulate in Benghazi twice asked for permission to\n",
      "help Ambassador Chris Stevens and twice were told to stand down.\n",
      "Furthermore sources present during the deadly six-hour assault have\n",
      "said that a desperate last request for military assistance once the\n",
      "CIA themselves came under attack was denied, even though elite\n",
      "counter-terrorism units were only two hours away. And it has been\n",
      "claimed there was full communication between the CIA annex in Benghazi\n",
      "and the U.S. military, casting further doubts on the Obama\n",
      "administration's assertion that there wasn't enough information to\n",
      "deploy forces - deepening the crisis over their handling of the attack\n",
      "on September 11th and its aftermath . Scroll down for video .\n",
      "Revelations: It has been claimed today that CIA operatives at the\n",
      "Benghazi consulate compound repeatedly had their requests for help\n",
      "denied during the deadly assault on September 11 . The lethal assault\n",
      "which led to the death of Ambassador Stevens and three other U.S.\n",
      "citizens began at 9.40 p.m. as the U.S Consulate came under fire from\n",
      "hostile Libyan forces. According to Fox News, ex-Navy SEAL Tyrone\n",
      "Woods, who was part of a small team at the CIA annex about a mile from\n",
      "the consulate, asked his superiors if he could go and assist the\n",
      "embattled diplomatic compound. However, they were told to 'stand down'\n",
      "and according to sources who spoke to the news channel were told to\n",
      "'stand down' again after asking for a second time to help Ambassador\n",
      "Stevens and his staff. Ignoring these orders, Woods and two others\n",
      "heroically made their way to the consulate which by now was ablaze and\n",
      "began firing on the attackers. Knowledge: It is unclear who denied the\n",
      "requests of the CIA for special forces teams . The quick reaction\n",
      "force which Woods was part of helped an evacuation of the main\n",
      "building and recovered the body of State Department staff member Sean\n",
      "Smith who had died in the initial attack. However, the team from the\n",
      "CIA annex could not locate Ambassador Stevens and returned to their\n",
      "own base at around midnight where they came under attack themselves.\n",
      "Immediately calling for assistance from Sigonella Air base in Italy\n",
      "which is two hours away, it is claimed that two separate special\n",
      "operations teams and air support were told to wait - despite the gun\n",
      "battle raging for four hours. It is not known who denied the request\n",
      "for help for the CIA operatives on the ground at Benghazi. If true\n",
      "these claims will radically change the perception of the field agents\n",
      "who were operating on the ground in Benghazi. Previously criticised\n",
      "for providing inadequate security for the consulate staff, the new\n",
      "information shows intelligence operatives repeatedly tried to assist\n",
      "and in fact were denied their own requests for outside help. Killed:\n",
      "Ambassador Christopher Stevens (left) died following smoke inhalation,\n",
      "while agent Sean Smith (right) died in a . desperate battle . Heroic:\n",
      "Former Navy SEALs Glen Doherty (left) and Tyrone Woods (right) were\n",
      "killed in a mortar attack . Refuting the Defense Secretary Leon\n",
      "Panetta's claim yesterday that there simply wasn't enough information\n",
      "to responsibly deploy forces to Libya at the time of the attack,\n",
      "sources on the ground claim that communication was open throughout the\n",
      "attack. Indeed, one member of the CIA team who was on the roof of the\n",
      "annex was in possession of a laser to guide aerial targets including\n",
      "drones and repeatedly requested backup from a Specter gunship to take\n",
      "out an attacker firing mortars. According to sources familiar with the\n",
      "situation, the operative had visual contact with the Libyan mortar\n",
      "team and in addition was able to pinpoint positions from where the\n",
      "consulate attackers were firing from. Yesterday Leon Panetta claimed\n",
      "that he and General Martin Dempsey, chairman of the U.S. military's\n",
      "Joint Chiefs of Staff, and General Carter Ham, head of the U.S.\n",
      "military's Africa Command, felt they couldn't 'put forces at risk in\n",
      "that situation.' A CIA operatives request for an AC-130H gunship to\n",
      "take down a Libyan mortar position was denied . Inferno: Armed\n",
      "attackers dumped cans of diesel fuel and set ablaze the consulate's\n",
      "exterior . Siege: The compound came under heavy mortar and gunfire\n",
      "during the attack, which lasted several hours . 'This happened within\n",
      "a few hours and it was really over before, you know, we had the\n",
      "opportunity to really know what was happening,' said Panetta.\n",
      "Furthermore, Fox News has learned that there were two military\n",
      "surveillance drones above the skies of Benghazi during the attack\n",
      "which would have been able to relay real time visuals of the assault\n",
      "to U.S. officials in the White House situation room and the Pentagon.\n",
      "Tyrone Woods and another former Navy SEAL Glen Doherty were killed by\n",
      "a mortar shell at 4 a.m. Libyan time, over six hours after the attack\n",
      "began and just one hour after relief from an American Quick Reaction\n",
      "Force sent from Tripoli had arrive. This new information comes as\n",
      "President Barack Obama's response to the attacks in Libya has become a\n",
      "contentious issue in the hard-fought U.S. presidential race, with\n",
      "Republican opponents raising questions about his administration's\n",
      "truthfulness and competence. Obama supporters have in turn accused\n",
      "Republicans of making unfounded accusations in an effort to score\n",
      "political points from the death of a U.S. ambassador and the three\n",
      "others killed in the Benghazi attack. Flames, grenades and gunfire: A\n",
      "burnt-out car in front of the U.S. consulate . The Speaker of the\n",
      "House of Representatives, Republican John Boehner, asked in a letter\n",
      "to Obama on Thursday about whether military options and assets were\n",
      "offered 'during and in the immediate aftermath of the terrorist\n",
      "attack.' 'Can you explain what options were presented to you or your\n",
      "staff, and why it appears assets were not allowed to be pre-\n",
      "positioned, let alone utilized?' Boehner asked. Defense Secretary Leon\n",
      "Panetta told Pentagon reporters that U.S. forces were on a heightened\n",
      "state of alert already because of the 11th anniversary of the\n",
      "September 11, 2001, attacks on New York and Washington by al Qaeda. In\n",
      "the aftermath of the attack, Panetta reminded reporters that the\n",
      "Pentagon deployed a Marine fleet anti-terrorist security team to\n",
      "Tripoli and had Navy ships off the coast. 'And we were prepared to\n",
      "respond to any contingency. And certainly had forces in place to do\n",
      "that,' he said. Elite team: As the U.S. consulate in Benghazi . came\n",
      "under a devastating attack last month, a rescue team of elite .\n",
      "soldiers was denied the opportunity to assist the CIA who had come\n",
      "under attack . The administration initially attributed the violence to\n",
      "protests over an anti-Islam film and said it was not premeditated.\n",
      "Obama and other officials have since said the incident was a\n",
      "deliberate terrorist attack. U.S. Secretary of State Hillary Clinton\n",
      "has attributed the shifting explanation to 'the fog of war.' A State\n",
      "Department email made public this week showed that two hours after the\n",
      "attack on the U.S. diplomatic mission compound in Benghazi, the\n",
      "Department's Operations Center advised officials at various U.S.\n",
      "agencies that a militant group called Ansar al-Sharia had claimed\n",
      "credit on Twitter and Facebook for the attacks. U.S. officials,\n",
      "including Clinton, on Wednesday said that such Internet postings did\n",
      "not constitute hard evidence of who was responsible for the attacks.\n",
      "The State Department has set up an independent review board to\n",
      "investigate the background and response to the attacks. The U.S.\n",
      "Senate intelligence committee on Thursday said it will hold hearings\n",
      "in November - after the November 6 presidential election - on security\n",
      "and intelligence issues raised by the September 11 attack in\n",
      "Libya.<EOS><pad>Revelationsshed new light on the effectiveness of the\n",
      "CIA at Benghazi and the level of support they were given . When the\n",
      "CIA annex come under attack the field agents were denied a request for\n",
      "military help despite a counter terrorism team being two hours away in\n",
      "Italy . There was full communication between operatives on the ground\n",
      "and headquarters - with the ability to laser guide drones, planes or\n",
      "special forces to enemy targets .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# print the article and its summary\n",
    "print('Article:\\n\\n', detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Attention Models from scatch\n",
    "\n",
    "### Dot Product Attention\n",
    "\n",
    "functions to create tensors and display useful information:\n",
    "\n",
    "create_tensor creates a jax numpy array from a list of lists.\n",
    "\n",
    "display_tensor prints out the shape and the actual tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    \n",
    "    return jnp.array(t)\n",
    "\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "_0x0HJXwRWQk",
    "outputId": "d6d78a8e-e3cc-47af-9584-2bdcdfcca0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "kVR9u4faRWQo",
    "outputId": "f01ea4ca-4152-4b54-b76a-e4b5917ae2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query dot key shape: (2, 2)\n",
      "\n",
      "[[0.57735026 2.309401  ]\n",
      " [1.1547005  2.8867514 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
    "display_tensor(q_dot_k, 'query dot key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key shape: (2, 2)\n",
      "\n",
      "[[ 5.7735026e-01  2.3094010e+00]\n",
      " [-1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked = q_dot_k + m\n",
    "display_tensor(masked, 'masked query dot key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key dot value shape: (2, 3)\n",
      "\n",
      "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
      " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(masked @ v, 'masked query dot key dot value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "key with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "\n",
      "value with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[0 1 0]\n",
      "  [1 0 1]]]\n",
      "\n",
      "boolean mask shape: (2, 2)\n",
      "\n",
      "[[ True  True]\n",
      " [False  True]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_with_batch = q[None,:]\n",
    "display_tensor(q_with_batch, 'query with batch dim')\n",
    "k_with_batch = k[None,:]\n",
    "display_tensor(k_with_batch, 'key with batch dim')\n",
    "v_with_batch = v[None,:]\n",
    "display_tensor(v_with_batch, 'value with batch dim')\n",
    "m_bool = create_tensor([[True, True], [False, True]])\n",
    "display_tensor(m_bool, 'boolean mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSauPt0NUl_o"
   },
   "outputs": [],
   "source": [
    "def DotProductAttention(query, key, value, mask):\n",
    "    \n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    depth = query.shape[-1] \n",
    "    \n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
    "    \n",
    "    if mask is not None: \n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "    \n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "\n",
    "    attention = jnp.matmul(dots, value)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8o0K7VWKRWQw",
    "outputId": "1c51af3a-5f11-480f-b33b-419072d8298c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
       "              [1.        , 0.        , 1.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query matrix (2D tensor) shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
      "\n",
      "[[[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]\n",
      "\n",
      "\n",
      " [[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]]\n",
      "\n",
      "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor2d = create_tensor(q)\n",
    "display_tensor(tensor2d, 'query matrix (2D tensor)')\n",
    "\n",
    "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
    "display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n",
    "\n",
    "tensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n",
    "\n",
    "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    \n",
    "    def compute_attention_heads(x):\n",
    "        \n",
    "        # Size of the x's batch dimension\n",
    "        batch_size = x.shape[0]\n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "       \n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
    "       \n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        \n",
    "        x = jnp.reshape(x, (-1, seqlen, d_head))\n",
    "        \n",
    "       \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "output tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(tensor3dc3b, \"input tensor\")\n",
    "result_cah = compute_attention_heads_closure(2,3)(tensor3dc3b)\n",
    "display_tensor(result_cah, \"output tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_self_attention(q, k, v):\n",
    "    \n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.        , 1.        , 0.        ],\n",
       "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "       \n",
    "        seqlen = x.shape[1]\n",
    "        \n",
    "        x = jnp.reshape(x, ( -1, n_heads, seqlen, d_head))\n",
    "      \n",
    "        x = jnp.transpose(x, ( 0, 2, 1 , 3))\n",
    "        \n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "output tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(result_cah, \"input tensor\")\n",
    "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
    "display_tensor(result_cao, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention Function\n",
    "\n",
    "Now it is time to put everything together within the `CausalAttention` or Masked multi-head attention function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9Adn6DtRWRG"
   },
   "outputs": [],
   "source": [
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "  \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
    "        \n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        \n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), \n",
    "        tl.Dense(d_feature) # Final dense layer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Branch_out3[\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "  ]\n",
      "  DotProductAttn_in3\n",
      "  AttnOutput\n",
      "  Dense_512\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the causal attention model\n",
    "print(CausalAttention(d_feature=512, n_heads=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKOxnRbp1K5U"
   },
   "outputs": [],
   "source": [
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \n",
    "    # Create masked multi-head attention block using CausalAttention function\n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    feed_forward = [ \n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer \n",
    "        tl.Dense(d_ff),\n",
    "        # Add activation function passed in as a parameter \n",
    "        ff_activation(), # Generally ReLU\n",
    "        # Add dropout with rate and mode specified \n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add second feed forward layer \n",
    "        tl.Dense(d_model),\n",
    "        # Add dropout with rate and mode specified \n",
    "        tl.Dropout(rate=dropout,mode=mode)\n",
    "    ]\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.LayerNorm(),\n",
    "          # Add causal attention block previously defined \n",
    "          causal_attention,\n",
    "          # Add dropout with rate and mode specified\n",
    "          tl.Dropout(rate=dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          # Add feed forward block\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Serial[\n",
      "        Branch_out3[\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "        ]\n",
      "        DotProductAttn_in3\n",
      "        AttnOutput\n",
      "        Dense_512\n",
      "      ]\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "], Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Dense_2048\n",
      "      Serial[\n",
      "        Relu\n",
      "      ]\n",
      "      Dropout\n",
      "      Dense_512\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the decoder block\n",
    "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yi4LJO1RWRS"
   },
   "outputs": [],
   "source": [
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "   \n",
    "    # Embedding inputs and positional encoder\n",
    "    positional_encoder = [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads,\n",
    "                    dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    return tl.Serial(\n",
    "        \n",
    "        tl.ShiftRight(mode=mode), \n",
    "        positional_encoder,\n",
    "        \n",
    "        decoder_blocks,\n",
    "        \n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        \n",
    "        tl.Dense(vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_33300_512\n",
      "  Dropout\n",
      "  PositionalEncoding\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Serial[\n",
      "          Branch_out3[\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "          ]\n",
      "          DotProductAttn_in3\n",
      "          AttnOutput\n",
      "          Dense_512\n",
      "        ]\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Dense_2048\n",
      "        Serial[\n",
      "          Relu\n",
      "        ]\n",
      "        Dropout\n",
      "        Dense_512\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  LayerNorm\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the Transformer\n",
    "print(TransformerLM(n_layers=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRRKnoAdvmJ7"
   },
   "source": [
    "#  Training\n",
    "\n",
    "Now we are going to train our model. As usual, we have to define the cost function, the optimizer, and decide whether we will be training it on a `gpu` or `cpu`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1lkVebQRWRV"
   },
   "source": [
    "We will now write a function that takes in our model and trains it. To train our model we have to decide how many times we want to iterate over the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM2gpu4xvjtX"
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
    "    \n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
    "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=10\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
    "    )\n",
    "\n",
    "    loop = training.Loop(TransformerLM(d_model=4,\n",
    "                                       d_ff=16,\n",
    "                                       n_layers=1,\n",
    "                                       n_heads=2,\n",
    "                                       mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "BFRBTwSqRWRZ",
    "outputId": "aff859e5-8f4a-4d3b-f1d3-98e137581a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 316336\n",
      "Step      1: Ran 1 train steps in 9.84 secs\n",
      "Step      1: train CrossEntropyLoss |  10.41285133\n",
      "Step      1: eval  CrossEntropyLoss |  10.41454029\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 61.95 secs\n",
      "Step     10: train CrossEntropyLoss |  10.41297340\n",
      "Step     10: eval  CrossEntropyLoss |  10.40849113\n",
      "Step     10: eval          Accuracy |  0.01063830\n"
     ]
    }
   ],
   "source": [
    "!rm -f ~/model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKrEBjmskeWa"
   },
   "source": [
    "\n",
    " ## Evaluation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model architecture\n",
    "model = TransformerLM(mode='eval')\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilM9C8P3RWRf"
   },
   "source": [
    "# Testing with our own input\n",
    "\n",
    "We will now test our input. We are going to implement greedy decoding. This consists of two functions. The first one allows to identify the next symbol. It gets the argmax of the output of our model and then returns that index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD_bXRCpRWRg"
   },
   "outputs": [],
   "source": [
    "def next_symbol(cur_output_tokens, model):\n",
    "    \n",
    "    # current output tokens length\n",
    "    token_length = len(cur_output_tokens)\n",
    "    \n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    \n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :]\n",
    "\n",
    "    \n",
    "    output, _ = model((padded_with_batch, padded_with_batch))  \n",
    "   \n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    \n",
    "    return int(np.argmax(log_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AwrQFglRWRj"
   },
   "source": [
    "\n",
    "### Greedy decoding\n",
    "\n",
    "Now we will implement the greedy_decode algorithm that will call the `next_symbol` function. It takes in the input_sentence, the trained model and returns the the decoded sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HwIdimiN0k2"
   },
   "outputs": [],
   "source": [
    "# UNQ_C10\n",
    "# Decoding functions.\n",
    "def greedy_decode(input_sentence, model):\n",
    "    \n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        # Get next symbol\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        # Append next symbol to original sentence\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        # Append next symbol to generated sentence\n",
    "        generated_output.append(cur_output)\n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "9kHuIDGW1sOr",
    "outputId": "2525ca2c-4625-47c0-8456-f75598581993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a sunny day when I went to the market to buy some flowers. But\n",
      "I only found roses, not tulips. \n",
      "\n",
      ":\n",
      ": I\n",
      ": I just\n",
      ": I just found\n",
      ": I just found ros\n",
      ": I just found roses\n",
      ": I just found roses,\n",
      ": I just found roses, not\n",
      ": I just found roses, not tu\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips.\n",
      ": I just found roses, not tulips.<EOS>\n",
      ": I just found roses, not tulips.<EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test it out on a sentence!\n",
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DYgX-mzjyUia",
    "outputId": "b901e164-48b3-4124-d21a-fe7443d15b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s the posing craze sweeping the U.S. after being brought to fame by\n",
      "skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert\n",
      "Pujols - and even Republican politician Rick Perry. But now four\n",
      "students at Riverhead High School on Long Island, New York, have been\n",
      "suspended for dropping to a knee and taking up a prayer pose to mimic\n",
      "Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,\n",
      "Tyler Carroll and Connor Carroll were all suspended for one day\n",
      "because the ‘Tebowing’ craze was blocking the hallway and presenting a\n",
      "safety hazard to students. Scroll down for video. Banned: Jordan\n",
      "Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured\n",
      "left) were all suspended for one day by Riverhead High School on Long\n",
      "Island, New York, for their tribute to Broncos quarterback Tim Tebow.\n",
      "Issue: Four of the pupils were suspended for one day because they\n",
      "allegedly did not heed to warnings that the 'Tebowing' craze at the\n",
      "school was blocking the hallway and presenting a safety hazard to\n",
      "students. \n",
      "\n",
      "Jordan\n",
      "Jordan Ful\n",
      "Jordan Fulcol\n",
      "Jordan Fulcoly\n",
      "Jordan Fulcoly,\n",
      "Jordan Fulcoly, Wayne\n",
      "Jordan Fulcoly, Wayne Dre\n",
      "Jordan Fulcoly, Wayne Drexe\n",
      "Jordan Fulcoly, Wayne Drexel\n",
      "Jordan Fulcoly, Wayne Drexel,\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not hee\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warn\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the '\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Te\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebow\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "cra\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocki\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hall\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test it out with a whole article!\n",
    "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "print(wrapper.fill(article), '\\n')\n",
    "print(greedy_decode(article, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
